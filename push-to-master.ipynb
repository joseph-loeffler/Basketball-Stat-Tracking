{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aY_uThqL5cg2yD8EahQi8BVGQF87_FIz",
      "authorship_tag": "ABX9TyPxXLyGSgQx+FZMlKmZbpa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseph-loeffler/Basketball-Stat-Tracking/blob/main/push-to-master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "# Project Title: Push to Master List\n",
        "## Project Overview\n",
        "This project is designed to transform and integrate financial data from various sources into a unified master sheet. It uses Google Colab, Google Sheets, and Google Drive APIs to authenticate, retrieve, process, and upload data. The data transformation involves mapping property GL codes to their corresponding QuickBooks names, reformatting raw data, and performing necessary data validations before uploading.\n",
        "\n",
        "## Features\n",
        "* **Google Sheets Integration**: The project uses Google Sheets to store and manage the data. It authenticates the user and authorizes the Google Sheets API for data access and manipulation.\n",
        "* **Data Transformation**: The script processes raw data, maps property GL codes to QuickBooks descriptions, reformats data, and performs necessary validations.\n",
        "* **Error Handling and Retries**: The script includes mechanisms to handle API quota limits and retry operations with exponential backoff.\n",
        "* **Automated Data Upload**: Processed data is uploaded to the master sheet, with checks to prevent duplicates and ensure data integrity.\n",
        "\n",
        "## Setup and Requirements\n",
        "* **Google Colab**: The script is designed to run in Google Colab. Ensure you have access to a Google Colab environment.\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Raw Source Files\n",
        "* **Ramp**\n",
        "    1. Go to [\"Ramp.com/home\" > \"Accounting\" > \"Ramp Card\"](https://app.ramp.com/accounting/ramp-card/needs-review).\n",
        "    2. Click \"Sync All\".\n",
        "    3. Click \"[name of button]\".\n",
        "* **Ramp Reimbursements**\n",
        "    1. Go to [\"Ramp.com/home\" > \"Accounting\" > \"Reimbursements\"](https://app.ramp.com/accounting/reimbursements/needs-review).\n",
        "    2. Click \"Sync All\".\n",
        "    3. Click \"[name of button]\".\n",
        "* **Old Lowe's *(Obsolete)***\n",
        "    1. Website not working rn...\n",
        "* **New Lowe's**\n",
        "    1. TBD.\n",
        "* **Amazon**\n",
        "    1. Go to [\"Amazon.com\" > \"Your Account\" > \"Business analytics\" > \"Reports\"](https://www.amazon.com/b2b/aba/reports?ref=hpr_redirect_your_reports).\n",
        "    2. Click on \"Create report\" and select \"Orders\" from the dropdown menu.\n",
        "    3. Click on \"Adjust columns\", and check the box next to all the column categories.\n",
        "    4. Click \"Submit\".\n",
        "    5. Click on the dropdown menu under \"Time period\" and select \"Custom Range\". Then, enter your desired start and end dates in their respective input boxes.\n",
        "    6. Click \"Generate report\".\n",
        "* **Delta Amex**\n",
        "    1. TBD.\n",
        "* **QLR**\n",
        "    1. In Google Drive, go to [Property Billback Invoices/Raw Source Files/Time/Marketing Bill Back.gsheet](https://docs.google.com/spreadsheets/d/1vkrNdk9OYIyH5KuVK01CCOvzn7KQhJEkwJ6b2Q23wGA/edit?usp=sharing).\n",
        "    2. Ensure that you are on the \"QLR\" sheet.\n",
        "    3. Click \"File\" > \"Download\" > \"Comma Separated Values (.csv)\"\n",
        "* **Digital Marketing**\n",
        "    1. In Google Drive, go to [Property Billback Invoices/Raw Source Files/Time/Marketing Bill Back.gsheet](https://docs.google.com/spreadsheets/d/1vkrNdk9OYIyH5KuVK01CCOvzn7KQhJEkwJ6b2Q23wGA/edit?gid=501220770#gid=501220770).\n",
        "    2. Ensure that you are on the \"Digital Marketing\" sheet.\n",
        "    3. Click \"File\" > \"Download\" > \"Comma Separated Values (.csv)\"\n",
        "* **JSQ**\n",
        "    1. In Google Drive, go to [Property Billback Invoices/Raw Source Files/JSQ/JSQ.gsheet](https://docs.google.com/spreadsheets/d/1-0BsuBs__ql-5c49pzgCuD19kqwDGDKye-kNl36GDIQ/edit?usp=drive_link).\n",
        "    2. Ensure that you are on the \"2024\" sheet.\n",
        "    3. Click \"File\" > \"Download\" > \"Comma Separated Values (.csv)\"\n",
        "* **Jeremy**\n",
        "    1. In Google Drive, go to [Property Billback Invoices/Raw Source Files/Jeremy/Sholler Time Billing.gsheet](https://docs.google.com/spreadsheets/d/1RQdivyTOo55ybzBVd62ASzuDOAT4ht9cbGWHHCE7OA4/edit?usp=drive_link).\n",
        "    2. Ensure that you are on the \"Sheet1\" sheet.\n",
        "    3. Click \"File\" > \"Download\" > \"Comma Separated Values (.csv)\"\n",
        "\n",
        "\n",
        "## Maintaining this Project\n",
        "* **Constants and global vars**: Make sure to update any constant values defined after the imports. This includes the column-name dictionaries, the property-names list, folder and file links, etc.\n",
        "    * Rarely, there are constants that are defined in functions, such as the SaasAnt mandatory columns\n",
        "* **When 2025 comes around**: The script is coded to work with the 2024 versions of many of the live sheets. The LIVE_SHEET_DATE_CUTOFF constant and the urls/sheet names to those live sheets will need to be updated eventually.\n",
        "\n",
        "## Notes\n",
        "* **\"spread\" vs. \"sheet\"**: I use \"spread\" to denote Google Sheets files, which may or may not include more than one worksheet. I use \"sheet\" to denote individual worksheets.\n",
        "\n",
        "## Conclusion\n",
        "This project streamlines the process of transforming and integrating financial data from multiple sources into a unified master sheet, leveraging Google Colab and Google APIs for efficient data management and processing."
      ],
      "metadata": {
        "id": "BtVq7pq6_6AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "# Library imports\n",
        "from datetime import datetime\n",
        "from functools import wraps\n",
        "from dateutil import parser\n",
        "from typing import List, Set\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Setup Google Drive API\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "\n",
        "# Authenticate user\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Build Drive service\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "\n",
        "# Mapping from Property GL Codes to their QuickBooks names\n",
        "GL_CODE_TO_QB_DICT = {\n",
        "    'N/A': 'N/A',\n",
        "    'n/a': 'N/A',\n",
        "    'NA': 'N/A',\n",
        "    'na': 'N/A',\n",
        "    '1510-020': 'Entity Expenses:Financing Fee',\n",
        "    '1910-800': 'Entity Expenses:Acquisition Fee',\n",
        "    '2110-500': 'Entity Expenses:A/P - OTHER',\n",
        "    '6210-000': 'Marketing:Advertising',\n",
        "    '6210-050': 'Marketing:Marketing Software Fees',\n",
        "    '6210-100': 'Marketing:PPC Advertising',\n",
        "    '6210-105': 'Marketing:Community Outreach/Events',\n",
        "    '6220-000': 'Marketing:Locator/Broker Fees',\n",
        "    '6220-005': 'Marketing:Resident Referral Fees',\n",
        "    '6250-010': 'Marketing:Resident Events/Retention',\n",
        "    '6250-020': 'Marketing:Other Marketing',\n",
        "    '6250-040': 'Marketing:Reputation Management',\n",
        "    '6250-100': 'Administrative:Prospect Credit Check Fees',\n",
        "    '6250-200': 'Administrative:Furniture Rental/Storage',\n",
        "    '6250-225': 'Administrative:Package Locker',\n",
        "    '6250-600': 'Marketing:Leasing Promotions/Incentives',\n",
        "    '6250-700': 'Administrative:Dues/Subscriptions',\n",
        "    '6310-500': 'Administrative:Office Equipment',\n",
        "    '6311-000': 'Administrative:Office Supplies',\n",
        "    '6320-000': 'Fixed Expenses:Property Management Fees',\n",
        "    '6340-000': 'Administrative:Legal Fees',\n",
        "    '6340-010': 'Administrative:Professional/Consulting Fees',\n",
        "    '6360-050': 'Administrative:Answering Service Fees',\n",
        "    '6360-100': 'Administrative:Office/Recreation Telecom Fees',\n",
        "    '6360-400': 'Administrative:Cell Phone',\n",
        "    '6370-100': 'Administrative:Postage and Mail',\n",
        "    '6370-200': 'Administrative:Bank Fees/Interest Expense',\n",
        "    '6370-300': 'Administrative:Travel/Mileage',\n",
        "    '6370-500': 'Administrative:Meals & Entertainment',\n",
        "    '6390-000': 'Administrative:Other Admin.',\n",
        "    '6390-010': \"Administrative:Renter's Insurance\",\n",
        "    '6390-065': 'Administrative:Software Subscription Fees',\n",
        "    '6390-100': 'Fixed Expenses:HOA Dues',\n",
        "    '6390-150': 'Administrative:Training & Education',\n",
        "    '6390-160': 'Administrative:Secret Shop Surveys',\n",
        "    '6515-000': 'Repairs & Maintenance:Cleaning',\n",
        "    '6519-000': 'Repairs & Maintenance:Pest Control Service',\n",
        "    '6520-000': 'Marketing:Signage/Brochures',\n",
        "    '6536-500': 'Repairs & Maintenance:Landscaping/Irrigation',\n",
        "    '6541-050': 'Repairs & Maintenance:Appliances',\n",
        "    '6541-200': 'Repairs & Maintenance:Electrical',\n",
        "    '6541-300': 'Repairs & Maintenance:Plumbing',\n",
        "    '6541-400': 'Repairs & Maintenance:Flooring',\n",
        "    '6541-600': 'Repairs & Maintenance:Lightning',\n",
        "    '6541-904': 'Repairs & Maintenance:Windows & Doors',\n",
        "    '6541-907': 'Repairs & Maintenance:Roof and Gutter',\n",
        "    '6541-908': 'Administrative:Employee Uniforms',\n",
        "    '6542-901': 'Repairs & Maintenance:Fire Protection',\n",
        "    '6542-909': 'Repairs & Maintenance:Gutter Cleaning',\n",
        "    '6542-910': 'Repairs & Maintenance:Carpentry and Finish Work',\n",
        "    '6542-911': 'Repairs & Maintenance:Security/Camera Systems',\n",
        "    '6543-900': 'Repairs & Maintenance:Dryer Vent/Chimney Cleaning',\n",
        "    '6545-050': 'Repairs & Maintenance:Elevator Repairs/Service Calls',\n",
        "    '6546-100': 'Repairs & Maintenance:HVAC',\n",
        "    '6547-050': 'Repairs & Maintenance:Pool',\n",
        "    '6560-110': 'Repairs & Maintenance:Painting',\n",
        "    '6561-100': 'Repairs & Maintenance:Window Coverings',\n",
        "    '6570-000': 'Repairs & Maintenance:Vehicle Repairs and Maintenance',\n",
        "    '6580-000': 'Repairs & Maintenance:Gates & Fences',\n",
        "    '6580-200': 'Repairs & Maintenance:Recreational Facilities',\n",
        "    '6581-000': 'Repairs & Maintenance:Garage/Parking',\n",
        "    '6590-000': 'Repairs & Maintenance:Other R&M',\n",
        "    '6619-000': 'Turnover:Pest Control',\n",
        "    '6642-908': 'Turnover:Tubs/Showers Repairs',\n",
        "    '6642-909': 'Repairs & Maintenance:Cabinet/Countertop Repairs',\n",
        "    '6647-000': 'Turnover:Cleaning',\n",
        "    '6647-100': 'Turnover:Carpet Cleaning',\n",
        "    '6660-100': 'Turnover:Touch-Up Painting',\n",
        "    '6690-000': 'Turnover:Trash Removal',\n",
        "    '6690-100': 'Turnover:Other Turnover',\n",
        "    '6710-000': 'Fixed Expenses:Real Estate Property Taxes',\n",
        "    '6710-020': 'Fixed Expenses:Property Tax - Other',\n",
        "    '6719-000': 'Fixed Expenses:Business Tax & License',\n",
        "    '6719-010': 'Fixed Expenses:Ground Lease',\n",
        "    '6719-100': 'Fixed Expenses:Personal Property Taxes',\n",
        "    '6720-000': 'Fixed Expenses:Property Insurance',\n",
        "    '6728-000': 'Fixed Expenses:Vehicle Insurance & Registration',\n",
        "    '7120-000': 'Entity Expenses:Legal Fees',\n",
        "    '7120-005': 'Entity Expenses:Marketing Fees',\n",
        "    '7120-015': 'Entity Expenses:Professional/Consulting Fees',\n",
        "    '7120-045': 'Professional Fees:Accounting/Tax Prep Fees',\n",
        "    '7120-200': 'Entity Expenses:Asset management fee',\n",
        "    '7190-000': 'Entity Expenses:Other Partnership Expenses',\n",
        "    '7190-003': 'Entity Expenses:Exec. Board Related',\n",
        "    '7190-020': 'Entity Expenses:Insurance Related',\n",
        "    '7190-400': 'Entity Expenses:Acquisition Costs',\n",
        "    '7290-900': 'Entity Expenses:Other Pre-Opening Expenses',\n",
        "    '7310-010': 'Non Unit Renovation:Concrete/Walkways',\n",
        "    '7310-100': 'Replacement:Building Exteriors',\n",
        "    '7310-205': 'Replacement:Elevators',\n",
        "    '7310-210': 'Replacement:Mirrors/Medicine Cabinets',\n",
        "    '7310-240': 'Replacement:Electrical Infrastructure',\n",
        "    '7310-250': 'Replacement:Office Equipment',\n",
        "    '7310-400': 'Replacement:Plumbing Fixtures',\n",
        "    '7315-020': 'Replacement:Tree Trimming',\n",
        "    '7320-208': 'Replacement:Concrete/Walkways',\n",
        "    '7320-300': 'Replacement:Exterior Lighting',\n",
        "    '7320-400': 'Replacement:Gates & Fences',\n",
        "    '7320-500': 'Replacement:Pool',\n",
        "    '7320-800': 'Replacement:Maintenance Equipment / Shop',\n",
        "    '7320-807': 'Replacement:Fire Protection',\n",
        "    '7325-000': 'Replacement:Recreation & Amenity Areas',\n",
        "    '7330-010': 'Replacement:Garages/Carports',\n",
        "    '7335-100': 'Replacement:Security Systems/Cameras',\n",
        "    '7338-000': 'Replacement:Solar',\n",
        "    '7355-100': 'Non Unit Renovation:Security Systems/Cameras',\n",
        "    '7362-000': 'Unit Renovation:Corporate Furniture',\n",
        "    '7365-010': 'Replacement:Golf Cart',\n",
        "    '7390-000': 'Non Unit Renovation:Access Control',\n",
        "    '7390-350': 'Non Unit Renovation:EV Charging',\n",
        "    '7391-900': 'Non Unit Renovation:Capital Maintenance Payroll',\n",
        "    '7502-023': 'Unit Renovation:Oversight/Mgmt. Fee',\n",
        "    '7505-000': 'Unit Renovation:Interior Doors/Closet Doors',\n",
        "    '7515-000': 'Unit Renovation:Painting Supplies & Labor',\n",
        "    '7520-000': 'Unit Renovation:Cabinets',\n",
        "    '7530-000': 'Unit Renovation:Interior Plumbing Fixtures',\n",
        "    '7538-000': 'Non Unit Renovation:Solar',\n",
        "    '7540-000': 'Unit Renovation:Countertops',\n",
        "    '7550-000': 'Unit Renovation:Tub/Shower Replacement',\n",
        "    '7556-000': 'Unit Renovation:Washers & Dryers',\n",
        "    '7565-000': 'Unit Renovation:Mirrors/Medicine Cabinets',\n",
        "    '7580-000': 'Unit Renovation:Carpets',\n",
        "    '7582-000': 'Unit Renovation:Vinyl/Hard Surface Flooring',\n",
        "    '7585-100': 'Unit Renovation:Dishwashers',\n",
        "    '7585-200': 'Unit Renovation:Stoves/Ranges',\n",
        "    '7585-300': 'Unit Renovation:Refrigerators',\n",
        "    '7585-400': 'Unit Renovation:Garbage Disposals',\n",
        "    '7590-000': 'Unit Renovation:Window Coverings',\n",
        "    '7591-000': 'Unit Renovation:Smart Home',\n",
        "    '7591-400': 'Unit Renovation:Interior Electrical Fixtures',\n",
        "    '7595-000': 'Unit Renovation:Other Interior Reno',\n",
        "    '7596-000': 'Unit Renovation:Permits/Fees',\n",
        "    '7597-000': 'Unit Renovation:Interior Framing/Drywall',\n",
        "    '7599-010': 'Non Unit Renovation:Rebates/Incentives',\n",
        "    '7605-000': 'Non Unit Renovation:New Construction',\n",
        "    '7615-000': 'Non Unit Renovation:Common Area Laundry',\n",
        "    '7615-100': 'Non Unit Renovation:Building Lobbies/Entryways',\n",
        "    '7625-100': 'Non Unit Renovation:Maintenance Equipment / Shop',\n",
        "    '7630-000': 'Non Unit Renovation:Asphalt/Parking Lot',\n",
        "    '7634-000': 'Non Unit Renovation:Office/Clubhouse',\n",
        "    '7634-200': 'Non Unit Renovation:Fitness Center',\n",
        "    '7635-000': 'Non Unit Renovation:Landscaping/Irrigation',\n",
        "    '7640-000': 'Non Unit Renovation:Exterior Painting',\n",
        "    '7641-000': 'Non Unit Renovation:HVAC',\n",
        "    '7643-000': 'Non Unit Renovation:Model Furniture',\n",
        "    '7655-000': 'Non Unit Renovation:Pool & Furniture',\n",
        "    '7655-200': 'Non Unit Renovation:Outdoor Bbq Area',\n",
        "    '7655-300': 'Non Unit Renovation:Sports Court',\n",
        "    '7655-500': 'Non Unit Renovation:Elevators',\n",
        "    '7655-750': 'Non Unit Renovation:Marketing',\n",
        "    '7655-900': 'Non Unit Renovation:Pet Wash Station',\n",
        "    '7660-000': 'Non Unit Renovation:Roofs',\n",
        "    '7661-000': 'Non Unit Renovation:Building Interior Hallways',\n",
        "    '7665-000': 'Non Unit Renovation:Signage',\n",
        "    '7665-800': 'Non Unit Renovation:Dog Park',\n",
        "    '7670-000': 'Non Unit Renovation:Exterior Siding',\n",
        "    '7675-000': 'Non Unit Renovation:Gates & Fences',\n",
        "    '7677-000': 'Non Unit Renovation:Green Improvements',\n",
        "    '7680-000': 'Non Unit Renovation:Playground',\n",
        "    '7685-000': 'Non Unit Renovation:Garages/Carports',\n",
        "    '7690-000': 'Non Unit Renovation:Oversight/Mgmt. Fee',\n",
        "    '7690-100': 'Non Unit Renovation:Design/Consulting/Professional Fees',\n",
        "    '7710-000': 'Replacement:Garbage Disposals',\n",
        "    '7710-100': 'Replacement:Dishwashers',\n",
        "    '7710-200': 'Replacement:Stoves/Ranges',\n",
        "    '7710-300': 'Replacement:Refrigerators',\n",
        "    '7710-400': 'Replacement:Washer & Dryers',\n",
        "    '7710-500': 'Replacement:Water Heaters',\n",
        "    '7715-000': 'Replacement:Cabinets/Countertops',\n",
        "    '7720-000': 'Replacement:Carpets',\n",
        "    '7725-000': 'Replacement:Vinyl/Hard Surface Flooring',\n",
        "    '7730-000': 'Replacement:Window Coverings',\n",
        "    '7740-000': 'Replacement:Interior Electrical Fixtures',\n",
        "    '7745-000': 'Replacement:HVAC',\n",
        "    '7764-000': 'Replacement:Signage',\n",
        "    '7765-000': 'Replacement:Landscaping & Irrigation',\n",
        "    '7766-000': 'Replacement:Asphalt/Parking Lot',\n",
        "    '7770-000': 'Replacement:Mail Boxes/Package Lockers',\n",
        "    '7775-000': 'Replacement:Other Replacement',\n",
        "    '7780-000': 'Replacement:Pest Control',\n",
        "    '7781-000': 'Replacement:Plumbing Infrastructure',\n",
        "    '7782-000': 'Replacement:Tubs/Showers',\n",
        "    '7783-000': 'Replacement:Gutters',\n",
        "    '7784-000': 'Replacement:Roof',\n",
        "    '7785-000': 'Replacement:Interior Painting',\n",
        "    '7785-100': 'Replacement:Carpentry and Finish Work',\n",
        "    '7786-000': 'Replacement:Windows/Doors',\n",
        "    '7786-500': 'Replacement:Keys & Access Control',\n",
        "    '7790-000': 'Replacement:Insurance Reimbursement',\n",
        "    '8212-015': 'Replacement:EV Charging',\n",
        "    '8212-035': 'Non Unit Renovation:Windows',\n",
        "    '8214-020': 'Non Unit Renovation:Exterior Doors',\n",
        "    '8216-050': 'Non Unit Renovation:Boilers/Water Heaters',\n",
        "    '8218-040': 'Non Unit Renovation:Plumbing Infrastructure',\n",
        "    '8218-044': 'Non Unit Renovation:Electrical Infrastructure',\n",
        "    '8220-010': 'Non Unit Renovation:Other Amenity/Recreation Area',\n",
        "    '8220-025': 'Non Unit Renovation:Exterior Lighting',\n",
        "    '8220-042': 'Non Unit Renovation:Drainage',\n",
        "    '8254-030': 'Marketing:Resident Move-In Gifts'}\n",
        "\n",
        "# Variable names used in code -> source column names\n",
        "col_names = {\n",
        "    'property name': {\n",
        "        'Master': 'Property Name',\n",
        "        'Ramp': 'QuickBooks Customer/Job',\n",
        "        'RampR': 'QuickBooks Customer/Job',\n",
        "        'Amazon': 'Custom Field 2',\n",
        "        'OldLowes': 'Property Name',\n",
        "        'QLR': 'Property Name',\n",
        "        'DigitalMarketing': 'Property Name',\n",
        "        'JSQ': 'Property Name',\n",
        "        'Jeremy': 'Property Name',\n",
        "        'DeltaAmex': 'Property Name'\n",
        "    },\n",
        "    'source':{\n",
        "        'Master': 'Source'\n",
        "    },\n",
        "    'GL code': {\n",
        "        'Master': 'Property GL Code',\n",
        "        'Ramp': 'Property GL Code',\n",
        "        'RampR': 'Property GL Code',\n",
        "        'Amazon': 'Property GL Code',\n",
        "        'OldLowes': 'Property GL Code',\n",
        "        'QLR': 'Property GL Code',\n",
        "        'DigitalMarketing': 'Property GL Code',\n",
        "        'JSQ': 'Property GL Code',\n",
        "        'Jeremy': 'Property GL Code',\n",
        "        'DeltaAmex': 'Property GL Code'\n",
        "    },\n",
        "    'GL code description': {\n",
        "        'Master': 'Property GL Code Description',\n",
        "        'Ramp': 'Property GL Code Description',\n",
        "        'RampR': 'Property GL Code Description',\n",
        "        'Amazon': 'Property GL Code Description',\n",
        "        'OldLowes': 'Property GL Code Description',\n",
        "        'QLR': 'Property GL Code Description',\n",
        "        'DigitalMarketing': 'Property GL Code Description',\n",
        "        'JSQ': 'Property GL Code Description',\n",
        "        'Jeremy': 'Property GL Code Description',\n",
        "        'DeltaAmex': 'Property GL Code Description'\n",
        "    },\n",
        "    'QB GL code description':{\n",
        "        'Master': 'Property GL Code Description (QuickBooks)'\n",
        "    },\n",
        "    'AI GL code': {\n",
        "        'Master': '(AI Predicted) Property GL Code',\n",
        "        'Ramp': '(AI Predicted) Property GL Code',\n",
        "        'RampR': '(AI Predicted) Property GL Code',\n",
        "        'Amazon': '(AI Predicted) Property GL Code',\n",
        "        'OldLowes': '(AI Predicted) Property GL Code',\n",
        "        'DeltaAmex': '(AI Predicted) Property GL Code'\n",
        "    },\n",
        "    'transaction date': {\n",
        "        'Master': 'Transaction Date',\n",
        "        'Ramp': 'Transaction Date',\n",
        "        'RampR': 'Transaction Date',\n",
        "        'Amazon': 'Order Date',\n",
        "        'OldLowes': 'Payment Date',\n",
        "        'QLR': 'Date',\n",
        "        'DigitalMarketing': 'Date',\n",
        "        'JSQ': 'Date',\n",
        "        'Jeremy': 'Date',\n",
        "        'DeltaAmex': 'Date'\n",
        "    },\n",
        "    'corporate account': {\n",
        "        'Master': 'Accounting Category',\n",
        "        'Ramp': 'QuickBooks Category',\n",
        "        'RampR': 'QuickBooks Category'\n",
        "    },\n",
        "    'transaction description': {\n",
        "        'Master': 'Item Name/Description',\n",
        "        'Amazon': 'Title',\n",
        "        'OldLowes': 'Expense Description',\n",
        "        'Jeremy': 'Project',\n",
        "        'DeltaAmex': 'Description'\n",
        "    },\n",
        "    'memo': {\n",
        "        'Master': 'Memo',\n",
        "        'Ramp': 'Memo',\n",
        "        'RampR': 'Memo',\n",
        "        'Amazon': 'Custom Field 4',\n",
        "        'OldLowes': 'Memo'\n",
        "    },\n",
        "    'debit': {\n",
        "        'Master': 'Debit',\n",
        "        'Ramp': 'Debit',\n",
        "        'RampR': 'Debit',\n",
        "        'Amazon': 'Item Net Total',\n",
        "        'OldLowes': 'Expense Line Amount',\n",
        "        'QLR': 'Amount',\n",
        "        'DigitalMarketing': 'Amount',\n",
        "        'JSQ': 'Amount',\n",
        "        'Jeremy': 'Amount',\n",
        "        'DeltaAmex': 'Amount'\n",
        "    },\n",
        "    'credit': {\n",
        "        'Master': 'Credit',\n",
        "        'Ramp': 'Credit',\n",
        "        'RampR': 'Credit'\n",
        "    },\n",
        "    'vendor': {\n",
        "        'Master': 'Vendor',\n",
        "        'Ramp': 'Merchant Name',\n",
        "        'RampR': 'Merchant Name'\n",
        "    },\n",
        "    'user': {\n",
        "        'Master': 'User',\n",
        "        'Ramp': 'Line Memo',\n",
        "        'RampR': 'Requester Name',\n",
        "        'Amazon': 'Account User',\n",
        "        'DeltaAmex': 'Card Member'\n",
        "    },\n",
        "    'SKU': {\n",
        "        'Master': 'SKU/ASIN',\n",
        "        'Amazon': 'ASIN',\n",
        "        'OldLowes': 'Expense Description'\n",
        "    },\n",
        "    'uploaded to saasant as expense?': {\n",
        "        'Master': 'Uploaded to SaasAnt as Expense?'\n",
        "    },\n",
        "    'uploaded to saasant as invoice?': {\n",
        "        'Master': 'Uploaded to SaasAnt as Invoice?'\n",
        "    },\n",
        "    'uploaded to saasant as bill?': {\n",
        "        'Master': 'Uploaded to SaasAnt as Bill?'\n",
        "    },\n",
        "    'unique id': {\n",
        "        'Master': 'Unique ID (from source)',\n",
        "        'Ramp': 'External ID',\n",
        "        'RampR': 'Reimbursement Link',\n",
        "        'Amazon': 'Order ID',\n",
        "        'OldLowes': 'Ref No',\n",
        "        'DeltaAmex': 'Reference'\n",
        "    },\n",
        "    'transaction link': {\n",
        "        'Master': 'Transaction Link',\n",
        "        'Ramp': 'Transaction Link',\n",
        "        'RampR': 'Reimbursement Link'\n",
        "    },\n",
        "    'Ramp category': {\n",
        "        'Master': 'Ramp Category',\n",
        "        'Ramp': 'Ramp Category'\n",
        "    },\n",
        "    'Amazon category': {\n",
        "        'Master': 'Amazon Category',\n",
        "        'Amazon': 'Commodity'\n",
        "    },\n",
        "    'DeltaAmex category': {\n",
        "        'Master': 'DeltaAmex Category',\n",
        "        'DeltaAmex': 'Category'\n",
        "    },\n",
        "    'Amazon payment ref id': {\n",
        "        'Master': 'Amazon Payment Reference ID',\n",
        "        'Amazon': 'Payment Reference ID'\n",
        "    },\n",
        "    'batch link': {\n",
        "        'Master': 'Link to all transactions'\n",
        "    },\n",
        "    'date pushed': {\n",
        "        'Master': 'Date Pushed to Master Sheet'\n",
        "    }}\n",
        "\n",
        "def build_dict(col_names):\n",
        "    \"\"\"\n",
        "    Builds a dictionary that maps source names to dictionaries of attribute-column name pairs.\n",
        "\n",
        "    Example:\n",
        "        Input:\n",
        "            col_names = {\n",
        "                'property name': {\n",
        "                    'Master': 'Property Name',\n",
        "                    'Ramp': 'QuickBooks Customer/Job'\n",
        "                },\n",
        "                'GL code': {\n",
        "                    'Master': 'Property GL Code',\n",
        "                    'Ramp': 'Property GL Code'\n",
        "                }\n",
        "            }\n",
        "\n",
        "        Output:\n",
        "            {\n",
        "                'Master': {\n",
        "                    'property name': 'Property Name',\n",
        "                    'GL code': 'Property GL Code'\n",
        "                },\n",
        "                'Ramp': {\n",
        "                    'property name': 'QuickBooks Customer/Job',\n",
        "                    'GL code': 'Property GL Code'\n",
        "                }\n",
        "            }\n",
        "    \"\"\"\n",
        "    source_specific_dict = {}\n",
        "    for attribute, sources in col_names.items():\n",
        "        for source, col_name in sources.items():\n",
        "            if source not in source_specific_dict:\n",
        "                source_specific_dict[source] = {}\n",
        "            source_specific_dict[source][attribute] = col_name\n",
        "\n",
        "    return source_specific_dict\n",
        "\n",
        "# Build the dictionary\n",
        "COL_NAMES = build_dict(col_names)\n",
        "\n",
        "READY_TO_UPLOAD_TO_MASTER_SHEET_FOLDER_URL = (\n",
        "    'https://drive.google.com/drive/folders/1Cchn82SQmZjtnsfUEoV3nXMldUtQoCOX')\n",
        "ARCHIVED_CODED_FILES_FOLDER_URL = (\n",
        "    'https://drive.google.com/drive/folders/1xFDMmzQax-Y2t8LHgUJ_7-XvY_wEuUHW')\n",
        "\n",
        "CORPORATE_PROPERTY_COA_MAPPING_SHEET = {\n",
        "    'url': ('https://docs.google.com/spreadsheets/d/1uc3oDxme39zJj9C-R92YohgV4l--MxesDaRz8bUvQo4/ed'\n",
        "            'it?gid=0#gid=0'),\n",
        "    'sheet title': 'Mapping'}\n",
        "\n",
        "MASTER_SHEET = {\n",
        "    'url': ('https://docs.google.com/spreadsheets/d/15nqM-3Jo-4t_JM7u_shRiBjXIIJ9JhokPyO5f6d1KoY/ed'\n",
        "            'it?gid=0#gid=0'),\n",
        "    'sheet title': 'Master List'}\n",
        "\n",
        "NON_BILLABLE_SHEET = {\n",
        "    'url': ('https://docs.google.com/spreadsheets/d/1XtrjNXQ8Spb0n6qzIubY3g7OeJ51CJxlm-1VZD8eReQ/ed'\n",
        "            'it?gid=0#gid=0'),\n",
        "    'sheet title': 'Non-Billable Transactions'}\n",
        "\n",
        "LIVE_SHEETS = {\n",
        "    'QLR': {\n",
        "        'url': ('https://docs.google.com/spreadsheets/d/1vkrNdk9OYIyH5KuVK01CCOvzn7KQhJEkwJ6b2Q23wG'\n",
        "                'A/edit?gid=0#gid=0'),\n",
        "        'sheet title': 'QLR',\n",
        "        'header row idx': 3,  # Google Sheets index (so 1-indexed)\n",
        "        'GL code': '6250-020',\n",
        "        'GL code description': 'Other Marketing'\n",
        "    },\n",
        "    'DigitalMarketing': {\n",
        "        'url': ('https://docs.google.com/spreadsheets/d/1vkrNdk9OYIyH5KuVK01CCOvzn7KQhJEkwJ6b2Q23wG'\n",
        "                'A/edit?gid=0#gid=0'),\n",
        "        'sheet title': 'DigitalMarketing',\n",
        "        'header row idx': 3,  # Google Sheets index (so 1-indexed)\n",
        "        'GL code': '6210-100',\n",
        "        'GL code description': 'PPC Advertising'\n",
        "    },\n",
        "    'JSQ': {\n",
        "        'url': ('https://docs.google.com/spreadsheets/d/1-0BsuBs__ql-5c49pzgCuD19kqwDGDKye-kNl36GDI'\n",
        "                'Q/edit?gid=969030513#gid=969030513'),\n",
        "        'sheet title': '2024',\n",
        "        'header row idx': 3,  # Google Sheets index (so 1-indexed)\n",
        "        'GL code': '7190-000',\n",
        "        'GL code description': 'Other Partnership Expenses'\n",
        "    },\n",
        "    'Jeremy': {\n",
        "        'url': ('https://docs.google.com/spreadsheets/d/1RQdivyTOo55ybzBVd62ASzuDOAT4ht9cbGWHHCE7OA'\n",
        "                '4/edit?gid=0#gid=0'),\n",
        "        'sheet title': 'Sheet1',\n",
        "        'header row idx': 1,  # Google Sheets index (so 1-indexed)\n",
        "        'GL code': pd.NA,\n",
        "        'GL code description': pd.NA\n",
        "    }}\n",
        "\n",
        "JEREMY_HOURLY_RATE = 100\n",
        "LIVE_SHEET_DATE_CUTOFF = '2024-01-01'\n",
        "RELOCATE_SPREAD_MODE = False\n",
        "MAX_API_RETRIES = 8  # Waits up to 128 seconds (for a total of 255 seconds)\n",
        "\n",
        "# Add property names after acquisitions\n",
        "PROPERTY_NAMES = [\n",
        "    'Battle Ground',            #1\n",
        "    'Briggs Village',           #2\n",
        "    'Clock Tower Village',      #3\n",
        "    'Crossings at Chapel Hill', #4\n",
        "    'Fern Ridge',               #5\n",
        "    'Lakeview Commons',         #6\n",
        "    'Laurel Creek',             #7\n",
        "    'Salish Flats',             #8\n",
        "    'Towncenter',               #9\n",
        "    'Township Canby',           #10\n",
        "    'Township Eastside',        #11\n",
        "    'Township Lake Meridian',   #12\n",
        "    'Township Sherwood',        #13\n",
        "    'Village'                   #14\n",
        "    #'Meadowscape'               #15\n",
        "    ]\n",
        "\n",
        "\n",
        "# General Google Drive API functions to load spreadsheets as dataframes,\n",
        "#  move files, get files info from folder, etc.\n",
        "def retry_on_quota_exceeded():\n",
        "    \"\"\"\n",
        "    Decorator to retry a function on gspread API quota exceeded errors (HTTP 429).\n",
        "    Retries the function with exponential backoff up to MAX_API_RETRIES times.\n",
        "\n",
        "    Returns:\n",
        "        function: The decorated function with retry logic.\n",
        "    \"\"\"\n",
        "    def decorator_retry(func):\n",
        "        @wraps(func)\n",
        "        def wrapper_retry(*args, **kwargs):\n",
        "            retry_count = 0\n",
        "            while retry_count < MAX_API_RETRIES:\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except gspread.exceptions.APIError as e:\n",
        "                    error = e.response.json()\n",
        "                    if error['error']['code'] == 429:\n",
        "                        # Handle quota exceeded error with exponential backoff\n",
        "                        retry_count += 1\n",
        "                        sleep_time = (2 ** retry_count) + (random.uniform(0, 1))\n",
        "                        print(f\"API quota exceeded, retrying in {sleep_time:.2f} seconds...\")\n",
        "                        time.sleep(sleep_time)\n",
        "                    else:\n",
        "                        raise  # Re-raise the exception if it's not a quota exceeded error\n",
        "            raise Exception(\"Max retries exceeded. Could not complete the request.\")\n",
        "        return wrapper_retry\n",
        "    return decorator_retry\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def open_sheet_from_url(spread_url, sheet_index=0, sheet_title=None):\n",
        "    \"\"\"Opens a Google Sheet by URL, returning a specific worksheet.\"\"\"\n",
        "    if sheet_title:\n",
        "        sheet = gc.open_by_url(spread_url).worksheet(sheet_title)\n",
        "    else:\n",
        "        sheet = gc.open_by_url(spread_url).get_worksheet(sheet_index)\n",
        "    return sheet\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def df_from_spread_url(spread_url, sheet_index=0, sheet_title=None):\n",
        "    \"\"\"Converts a Google Sheet to a pandas DataFrame.\"\"\"\n",
        "    sheet = open_sheet_from_url(spread_url, sheet_index, sheet_title)\n",
        "    return pd.DataFrame(sheet.get_all_records())\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def df_from_csv_url(url, header_row_idx=None, index_col_idx=None):\n",
        "    \"\"\"Converts a CSV file from a Google Drive URL to a pandas DataFrame.\"\"\"\n",
        "    file_id = extract_drive_id(url)\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f'/tmp/{file_id}.csv'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        downloader = MediaIoBaseDownload(f, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "    return pd.read_csv(file_path, header=header_row_idx, index_col=index_col_idx)\n",
        "\n",
        "\n",
        "def df_from_file_url(url, filetype):\n",
        "    \"\"\"Converts a csv or Google Sheets from a Google Drive URL to a pandas DataFrame.\"\"\"\n",
        "    if filetype == 'text/csv':\n",
        "        df = df_from_csv_url(url)\n",
        "    elif filetype == 'application/vnd.google-apps.spreadsheet':\n",
        "        df = df_from_spread_url(url)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown file type: {filetype}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def get_cols_by_name_from_sheet(sheet, column_names, header_row=1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get multiple columns in a pd.DataFrame from a Google Sheet based on the column names.\n",
        "\n",
        "    Parameters:\n",
        "        sheet (gspread.Sheet): The Google Sheet object.\n",
        "        column_names (list): A list of column names to fetch.\n",
        "        header_row (int): The row number of the header (1-indexed).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the columns.\n",
        "        None: If any column is not found.\n",
        "    \"\"\"\n",
        "    # Get the header row\n",
        "    header = sheet.row_values(header_row)\n",
        "\n",
        "    # Find the indices of the columns\n",
        "    col_indices = []\n",
        "    for column_name in column_names:\n",
        "        try:\n",
        "            col_idx = header.index(column_name) + 1\n",
        "            col_indices.append((column_name, col_idx))\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Column '{column_name}' not found in the sheet.\")\n",
        "\n",
        "    # Get all values in the specified columns, including the header\n",
        "    data = {}\n",
        "    for col_name, col_idx in col_indices:\n",
        "        column_values = sheet.col_values(col_idx)\n",
        "        data[col_name] = column_values[1:]  # Skip the header\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_drive_id(url):\n",
        "    \"\"\"Extracts the Google Drive file ID from various types of URLs.\"\"\"\n",
        "    # Define a regular expression pattern to match the ID in various types of Google Drive URLs\n",
        "    pattern = r'(?:drive/(?:folders|file|d)/|docs/(?:spreadsheets/d/))([a-zA-Z0-9-_]+)'\n",
        "    match = re.search(pattern, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        # Another attempt to match \"spreadsheets\" and \"file\" type URLs specifically\n",
        "        pattern_spreadsheets = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'\n",
        "        match_spreadsheets = re.search(pattern_spreadsheets, url)\n",
        "        if match_spreadsheets:\n",
        "            return match_spreadsheets.group(1)\n",
        "\n",
        "        pattern_file = r'/file/d/([a-zA-Z0-9-_]+)'\n",
        "        match_file = re.search(pattern_file, url)\n",
        "        if match_file:\n",
        "            return match_file.group(1)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def get_files_info_from_folder(folder_url):\n",
        "    \"\"\"\n",
        "    Retrieves information about files in a Google Drive folder.\n",
        "\n",
        "    Args:\n",
        "        folder_url (str): The URL of the Google Drive folder.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of dictionaries containing file information,\n",
        "                    each with 'name', 'url', and 'type' keys.\n",
        "    \"\"\"\n",
        "    folder_id = extract_drive_id(folder_url)\n",
        "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
        "    results = drive_service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, webViewLink, mimeType)\"\n",
        "    ).execute()\n",
        "    items = results.get('files', [])\n",
        "\n",
        "    files_info = []\n",
        "    for item in items:\n",
        "        files_info.append({\n",
        "            'name': item['name'],\n",
        "            'url': item['webViewLink'],\n",
        "            'type': item['mimeType']\n",
        "        })\n",
        "    return files_info\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def move_file(file_url, dst_folder_url):\n",
        "    \"\"\"\n",
        "    Moves a Google Drive file to a specified folder.\n",
        "\n",
        "    Args:\n",
        "        file_url (str): The URL of the Google Drive file.\n",
        "        dst_folder_url (str): The URL of the destination Google Drive folder.\n",
        "    \"\"\"\n",
        "    # Extract folder ID from destination folder URL\n",
        "    dst_folder_id = extract_drive_id(dst_folder_url)\n",
        "\n",
        "    # Extract file ID from file URL\n",
        "    file_id = extract_drive_id(file_url)\n",
        "\n",
        "    # Retrieve the current parents to remove them\n",
        "    file = drive_service.files().get(fileId=file_id, fields='parents').execute()\n",
        "    previous_parents = \",\".join(file.get('parents'))\n",
        "\n",
        "    # Move the file to the new folder\n",
        "    drive_service.files().update(\n",
        "        fileId=file_id,\n",
        "        addParents=dst_folder_id,\n",
        "        removeParents=previous_parents,\n",
        "        fields='id, parents'\n",
        "    ).execute()\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def append_df_to_sheet(df, dst_sheet):\n",
        "    \"\"\"Appends pandas DataFrame to Google Sheet\"\"\"\n",
        "    # Get the column names of the destination sheet\n",
        "    sheet_cols = dst_sheet.row_values(1)\n",
        "    df_cols = list(df.columns)\n",
        "\n",
        "    # Only keep columns in df that exist in the destination sheet\n",
        "    non_dst_cols = [col for col in df_cols if col not in sheet_cols]\n",
        "    df = df.drop(columns=non_dst_cols)\n",
        "\n",
        "    # Reindex df to match the order of columns in the destination sheet\n",
        "    df = df.reindex(columns=sheet_cols)\n",
        "\n",
        "    # Replace null values with an empty string\n",
        "    df.fillna('', inplace=True)\n",
        "\n",
        "    # Append rows to the destination sheet\n",
        "    dst_sheet.append_rows(df.values.tolist())\n",
        "\n",
        "\n",
        "# Functions that are used to reformat and transform the raw\n",
        "# source files into the master sheet formatting\n",
        "def uniquify_ids(df, id_col):\n",
        "    \"\"\"\n",
        "    Ensures unique values in a specified ID column by appending an index to duplicates.\n",
        "    Used on sources which already have some sort of unique(ish) ID.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the IDs.\n",
        "        id_col (str): The column name with the IDs to uniquify.\n",
        "    \"\"\"\n",
        "    # Count the occurrences of each ID\n",
        "    id_counts = df[id_col].value_counts()\n",
        "\n",
        "    # Create a dictionary to track the number of times an ID has been encountered\n",
        "    id_tracker = {key: 0 for key in id_counts.index if id_counts[key] > 1}\n",
        "\n",
        "    # Iterate over the DataFrame and modify the IDs as necessary\n",
        "    for index, row in df.iterrows():\n",
        "        id_value = row[id_col]\n",
        "        if id_counts[id_value] > 1:\n",
        "            df.at[index, id_col] = f\"{id_value}_{id_tracker[id_value]}\"\n",
        "            id_tracker[id_value] += 1\n",
        "\n",
        "\n",
        "def generate_id_column(df, cols):\n",
        "    \"\"\"\n",
        "    Generates a unique ID column by concatenating specified columns.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the data.\n",
        "        cols (List[str]): The column names to concatenate for the unique ID.\n",
        "\n",
        "    \"\"\"\n",
        "    id_col = COL_NAMES['Master']['unique id']\n",
        "\n",
        "    # Capitalize and join the strings, keeping fully capitalized and CamelCase words unchanged\n",
        "    def process_value(val, col_name):\n",
        "        if col_name == COL_NAMES['Master']['transaction date']:\n",
        "            val = val.replace('/', '-')\n",
        "        words = val.split()\n",
        "        processed_words = []\n",
        "        for word in words:\n",
        "            if word.isupper() or (word[0].isupper() and not word[1:].islower()):  # handle CamelCase\n",
        "                processed_words.append(word)\n",
        "            else:\n",
        "                processed_words.append(word.capitalize())\n",
        "        return ''.join(processed_words)\n",
        "\n",
        "    df[id_col] = df.apply(\n",
        "        lambda row: '_'.join(process_value(str(row[col]), col) for col in cols),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "\n",
        "def str_to_float(s):\n",
        "    \"\"\"Converts a string to a float.\"\"\"\n",
        "    zero_strings = ['', 'nan', 'null', 'na', 'n/a', 'zero']\n",
        "    if s.lower() in zero_strings:\n",
        "        return 0.0\n",
        "    s = s.replace(',', '')\n",
        "    s = s.replace('$', '')\n",
        "    s = s.replace(' ', '')\n",
        "    s = s.strip()\n",
        "    return float(s)\n",
        "\n",
        "\n",
        "def convert_to_mmddyyyy(date: str):\n",
        "    \"\"\"Converts a date string to MM/DD/YYYY format.\"\"\"\n",
        "    try:\n",
        "        parsed_date = parser.parse(date)\n",
        "        return parsed_date.strftime('%m/%d/%Y')\n",
        "    except (parser.ParserError, TypeError, ValueError):\n",
        "        raise ValueError(f\"Invalid date format: {date}\")\n",
        "\n",
        "\n",
        "def format_live_sheet(df, source):\n",
        "    \"\"\"\n",
        "    Formats a DataFrame from a live Google Sheet based on the source type. Much of the reformatting\n",
        "    has to do with melting the live sheets so that each row is a separate transaction.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to format.\n",
        "        source (str): The source type (e.g., 'DigitalMarketing', 'JSQ', 'Jeremy').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The formatted DataFrame.\n",
        "    \"\"\"\n",
        "    header_row_idx = LIVE_SHEETS[source]['header row idx'] - 1  # Set to 0-indexed\n",
        "\n",
        "    # Drop the rows above the header row (inclusive) and reset the index\n",
        "    df.drop(index=range(header_row_idx), inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Set the new column names to the header row\n",
        "    df.columns = df.iloc[0]\n",
        "\n",
        "    # Drop the header row\n",
        "    df = df[1:].reset_index(drop=True)\n",
        "\n",
        "    # Drop totals from DigitalMarketing\n",
        "    if source == 'DigitalMarketing':\n",
        "        total_idx = df[df[COL_NAMES[source]['property name']].astype(str).str.contains('Total billings')].index\n",
        "        if not total_idx.empty:\n",
        "            df.drop(index=range(total_idx[0], len(df)), inplace=True)\n",
        "\n",
        "    # Drop totals from JSQ\n",
        "    if source == 'JSQ':\n",
        "        last_valid_idx = df[COL_NAMES[source]['property name']].last_valid_index()\n",
        "        # Select rows up to the last valid index and drop the '2024 Total' column in place\n",
        "        df.drop(df.index[last_valid_idx+1:], inplace=True)\n",
        "        df.drop(columns=['2024 Total'], inplace=True)\n",
        "\n",
        "    # Drop totals from Jeremy\n",
        "    if source == 'Jeremy':\n",
        "        last_valid_idx = df[COL_NAMES[source]['property name']].last_valid_index()\n",
        "        # Drop rows after the last valid index\n",
        "        df.drop(df.index[last_valid_idx + 1:], inplace=True)\n",
        "\n",
        "    # Convert column names to datetime objects\n",
        "    date_cols = [col for col in df.columns if col != COL_NAMES[source]['property name']]\n",
        "    if source == 'Jeremy':\n",
        "        date_cols.pop(date_cols.index(COL_NAMES[source]['transaction description']))\n",
        "        date_cols.pop(date_cols.index(COL_NAMES[source]['GL code']))\n",
        "        date_cols.pop(date_cols.index(COL_NAMES[source]['GL code description']))\n",
        "\n",
        "    df.columns = [pd.to_datetime(col, format='%m/%d/%y', errors='raise') if col in date_cols else col for col in df.columns]\n",
        "\n",
        "    # Massage the df into a format where 'Date', 'Property Name', and 'Amount' are columns (and Project for Jeremy)\n",
        "    if source == 'Jeremy':\n",
        "        id_vars = [\n",
        "            COL_NAMES[source]['property name'],\n",
        "            COL_NAMES[source]['transaction description'],\n",
        "            COL_NAMES[source]['GL code'],\n",
        "            COL_NAMES[source]['GL code description']\n",
        "        ]\n",
        "    else:\n",
        "        id_vars = [COL_NAMES[source]['property name']]\n",
        "\n",
        "    melted_df = pd.melt(\n",
        "        df,\n",
        "        id_vars=id_vars,\n",
        "        var_name=COL_NAMES[source]['transaction date'],\n",
        "        value_name=COL_NAMES[source]['debit']\n",
        "    )\n",
        "\n",
        "    melted_df[COL_NAMES[source]['debit']] = melted_df[COL_NAMES[source]['debit']].astype(str).apply(str_to_float)\n",
        "\n",
        "    if source != 'Jeremy':\n",
        "        # Set the day to the last day of the month\n",
        "        melted_df[COL_NAMES[source]['transaction date']] += pd.offsets.MonthEnd(0)\n",
        "\n",
        "    # Get the current date\n",
        "    now = datetime.now()\n",
        "    current_month_start = now.replace(day=1)\n",
        "    next_month_start = (current_month_start + pd.offsets.MonthEnd(1))\n",
        "\n",
        "    # Keep transactions after the cutoff date, and before the start of the current month\n",
        "    condition = (\n",
        "        (melted_df[COL_NAMES[source]['transaction date']] >= LIVE_SHEET_DATE_CUTOFF) &\n",
        "        (melted_df[COL_NAMES[source]['transaction date']] < current_month_start)\n",
        "    )\n",
        "\n",
        "    # Identify the indices of rows to drop\n",
        "    rows_to_drop = melted_df.index[~condition]\n",
        "\n",
        "    # Identify rows in the current month\n",
        "    current_month_condition = (\n",
        "        (melted_df[COL_NAMES[source]['transaction date']] >= current_month_start)\n",
        "    )\n",
        "    current_month_rows = melted_df.loc[current_month_condition]\n",
        "\n",
        "    # Filter out rows with the \"debit\" column as zero\n",
        "    current_month_rows = current_month_rows[current_month_rows[COL_NAMES[source]['debit']] != 0]\n",
        "\n",
        "    # Print the (nonzero) rows in the current month that are about to be omitted\n",
        "    if not current_month_rows.empty:\n",
        "        print(\"Rows in the current month that are being omitted:\")\n",
        "        print(current_month_rows)\n",
        "\n",
        "    # Drop the rows in place\n",
        "    melted_df.drop(rows_to_drop, inplace=True)\n",
        "\n",
        "    if source == 'Jeremy':\n",
        "        return melted_df\n",
        "\n",
        "    # Group by poperty name and date and sum the amounts (except for Jeremy)\n",
        "    grouped_df = melted_df.groupby(\n",
        "        [COL_NAMES[source]['property name'], COL_NAMES[source]['transaction date']],\n",
        "        as_index=False\n",
        "    ).sum()\n",
        "\n",
        "    # Set GL Codes (except for Jeremy)\n",
        "    grouped_df[COL_NAMES[source]['GL code']] = LIVE_SHEETS[source]['GL code']\n",
        "    grouped_df[COL_NAMES[source]['GL code description']] = LIVE_SHEETS[source]['GL code description']\n",
        "\n",
        "    return grouped_df\n",
        "\n",
        "\n",
        "def general_formatting(df, source):\n",
        "    \"\"\"\n",
        "    Some raw columns are exactly what we need, so we can just port those over exactly as they are.\n",
        "    Other columns receive the same general transformation, regardless of source. The below code\n",
        "    implements this. However, many raw columns need some sort of manipulation, and these are handled\n",
        "    in the specific source_to_master functions.\n",
        "\n",
        "    This function creates many new columns, all w/ master sheet titles.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to format.\n",
        "        source (str): The source type.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a source column is not found in the DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The formatted DataFrame.\n",
        "    \"\"\"\n",
        "    for col_var, source_col in COL_NAMES[source].items():\n",
        "        if source_col in df.columns:\n",
        "            df[COL_NAMES['Master'][col_var]] = df[source_col]\n",
        "        else:\n",
        "            raise KeyError(f\"Source column {source_col} not found in dataframe.\")\n",
        "\n",
        "    df[COL_NAMES['Master']['source']] = source\n",
        "    df[COL_NAMES['Master']['QB GL code description']] = df[COL_NAMES['Master']['GL code']].map(GL_CODE_TO_QB_DICT)\n",
        "    df[COL_NAMES['Master']['transaction date']] = df[COL_NAMES['Master']['transaction date']].astype(str).apply(convert_to_mmddyyyy)\n",
        "    df[COL_NAMES['Master']['corporate account']] = df[COL_NAMES['Master']['GL code']].map(property_code_to_corporate_acc)\n",
        "    df[COL_NAMES['Master']['uploaded to saasant as expense?']] = False\n",
        "    df[COL_NAMES['Master']['uploaded to saasant as invoice?']] = False\n",
        "    df[COL_NAMES['Master']['uploaded to saasant as bill?']] = False\n",
        "\n",
        "    now = datetime.now()\n",
        "    date_string = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
        "    df[COL_NAMES['Master']['date pushed']] = date_string\n",
        "\n",
        "    for master_col in [COL_NAMES['Master']['debit'], COL_NAMES['Master']['credit']]:\n",
        "        if master_col in df.columns:\n",
        "            df[master_col] = df[master_col].astype(str).apply(str_to_float)\n",
        "        else:\n",
        "            df[master_col] = 0.0\n",
        "\n",
        "    # Remove transactions where Debit = Credit = 0\n",
        "    rows_to_drop = df.index[(df[COL_NAMES['Master']['debit']] == 0.0) & (df[COL_NAMES['Master']['credit']] == 0.0)]\n",
        "    df.drop(rows_to_drop, inplace=True)\n",
        "\n",
        "    # Create Unique IDs\n",
        "    # For live sheets, create a new unique ID\n",
        "    if source in LIVE_SHEETS.keys():\n",
        "        # Columns to be concatenated\n",
        "        id_cols = [\n",
        "            COL_NAMES['Master']['source'],\n",
        "            COL_NAMES['Master']['property name'],\n",
        "            COL_NAMES['Master']['transaction date']\n",
        "        ]\n",
        "        # Add project description for Jeremy\n",
        "        if source == 'Jeremy':\n",
        "            id_cols.insert(2, COL_NAMES['Master']['transaction description'])\n",
        "\n",
        "        generate_id_column(df, id_cols)\n",
        "\n",
        "    # Otherwise, uniquify existing ID\n",
        "    else:\n",
        "        # Convert IDs to strings\n",
        "        df[COL_NAMES['Master']['unique id']] = df[COL_NAMES['Master']['unique id']].astype(str)\n",
        "        # Run uniquify_ids function\n",
        "        uniquify_ids(df, COL_NAMES['Master']['unique id'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def ramp_to_master(df):\n",
        "    \"\"\"Implements Ramp-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['user']] = df[COL_NAMES['Master']['user']].apply(lambda x: x.split(\" - \")[0])\n",
        "\n",
        "\n",
        "def ramp_reimbursements_to_master(df):\n",
        "    \"\"\"Implements Ramp reimbursement-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['unique id']] = df[COL_NAMES['Master']['unique id']].apply(lambda x: x.split('/')[-1])\n",
        "    # Rerun uniquify_ids function\n",
        "    uniquify_ids(df, COL_NAMES['Master']['unique id'])\n",
        "\n",
        "\n",
        "def amazon_to_master(df):\n",
        "    \"\"\"Implements Amazon-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['vendor']] = 'Amazon'\n",
        "\n",
        "\n",
        "def old_lowes_to_master(df):\n",
        "    \"\"\"Implements Old Lowe's-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['memo']] = df[COL_NAMES['Master']['memo']].astype(str)\n",
        "    df[COL_NAMES['Master']['vendor']] = 'Lowes'\n",
        "    df[COL_NAMES['Master']['SKU']] = df[COL_NAMES['Master']['SKU']].apply(lambda x: x.split(':')[-1])\n",
        "\n",
        "\n",
        "def qlr_to_master(df):\n",
        "    \"\"\"Implements QLR-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['transaction description']] = \"QLR Billback\"\n",
        "\n",
        "\n",
        "def digital_marketing_to_master(df):\n",
        "    \"\"\"Implements Digital Marketing-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['transaction description']] = \"DigitalMarketing Billback\"\n",
        "\n",
        "\n",
        "def jsq_to_master(df):\n",
        "    \"\"\"Implements JSQ-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['transaction description']] = \"JSQ Billback\"\n",
        "\n",
        "\n",
        "def jeremy_to_master(df):\n",
        "    \"\"\"Implements Jeremy-specific data transformations\"\"\"\n",
        "    df[COL_NAMES['Master']['transaction description']] = \"Jeremy Billback\"\n",
        "    df[COL_NAMES['Master']['debit']] *= JEREMY_HOURLY_RATE\n",
        "\n",
        "\n",
        "def delta_amex_to_master(df):\n",
        "    \"\"\"Implements Delta Amex-specific data transformations\"\"\"\n",
        "    # source = Amex\n",
        "    return df\n",
        "\n",
        "\n",
        "def source_to_master(df, source):\n",
        "    \"\"\"Applies transformations to the DataFrame based on the source.\"\"\"\n",
        "    df_transformations = {\n",
        "        'Ramp': ramp_to_master,\n",
        "        'RampR': ramp_reimbursements_to_master,\n",
        "        'Amazon': amazon_to_master,\n",
        "        'OldLowes': old_lowes_to_master,\n",
        "        'QLR': qlr_to_master,\n",
        "        'DigitalMarketing': digital_marketing_to_master,\n",
        "        'JSQ': jsq_to_master,\n",
        "        'Jeremy': jeremy_to_master,\n",
        "        'DeltaAmex': delta_amex_to_master\n",
        "    }\n",
        "\n",
        "    if source in df_transformations:\n",
        "        return df_transformations[source](df)\n",
        "    else:\n",
        "        raise KeyError(f\"Unknown source: {source}\")\n",
        "\n",
        "\n",
        "# Functions that catch mistakes, raise errors, recognize missing data,\n",
        "# and implement safety features more generally\n",
        "def get_missing_raw_cols(df, source):\n",
        "    \"\"\"\n",
        "    Identifies columns expected in the DataFrame based on the source but\n",
        "    that are missing from the actual DataFrame.\n",
        "    \"\"\"\n",
        "    wanted_cols = set(COL_NAMES[source].values())\n",
        "    missing_cols = wanted_cols - set(df.columns)\n",
        "\n",
        "    return sorted(missing_cols)\n",
        "\n",
        "\n",
        "def get_missing_saasant_data(df, source):\n",
        "    \"\"\"\n",
        "    Identifies missing necessary data for SaasAnt export based on the source type.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to check.\n",
        "        source (str): The source type.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are column names and values are lists of row indices\n",
        "              (1-based) where data is missing.\n",
        "    \"\"\"\n",
        "    necessary_saasant_cols = [\n",
        "        COL_NAMES['Master']['property name'],\n",
        "        COL_NAMES['Master']['QB GL code description'],\n",
        "        COL_NAMES['Master']['transaction date'],\n",
        "        COL_NAMES['Master']['debit'],\n",
        "        COL_NAMES['Master']['credit'],\n",
        "    ]\n",
        "\n",
        "    if source == 'Amazon':\n",
        "        necessary_saasant_cols.append(COL_NAMES['Master']['Amazon payment ref id'])\n",
        "\n",
        "    missing_data = {}\n",
        "    for col in necessary_saasant_cols:\n",
        "        empty_value_indices = df[(df[col] == \"\") | (df[col].isnull())].index.tolist()\n",
        "        if empty_value_indices:\n",
        "            missing_data[col] = empty_value_indices\n",
        "    for key, val in missing_data.items():\n",
        "        missing_data[key] = [x + 2 for x in val]\n",
        "    return missing_data\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def get_duplicates(df, dst_sheet_list):\n",
        "    \"\"\"\n",
        "    Identifies duplicate unique IDs in the DataFrame compared to a list of destination sheets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to check for duplicates.\n",
        "        dst_sheet_list (list): A list of gspread worksheets to check against.\n",
        "\n",
        "    Returns:\n",
        "        list: A sorted list of duplicate unique IDs.\n",
        "    \"\"\"\n",
        "    id_col_name = COL_NAMES['Master']['unique id']\n",
        "    existing_unique_ids = set()\n",
        "\n",
        "    for dst_sheet in dst_sheet_list:\n",
        "        # Convert all values to strings before updating the set\n",
        "        col_values = [str(val) for val in dst_sheet.col_values(dst_sheet.find(id_col_name).col)[1:]]\n",
        "        existing_unique_ids.update(col_values)\n",
        "\n",
        "    duplicates = existing_unique_ids.intersection(set(df[id_col_name]))\n",
        "\n",
        "    return sorted(duplicates)\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def get_differing_transactions(src_df, dst_sheet_list):\n",
        "    \"\"\"\n",
        "    Identifies duplicate transactions in the source DataFrame (by unique id) that differ\n",
        "    from those in the destination sheets (by their debit/credit amounts).\n",
        "\n",
        "    Args:\n",
        "        src_df (pd.DataFrame): The source DataFrame.\n",
        "        dst_sheet_list (list): A list of gspread worksheets to compare against.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of transactions where debit or credit amounts differ.\n",
        "    \"\"\"\n",
        "    id_col = COL_NAMES['Master']['unique id']\n",
        "    debit_col = COL_NAMES['Master']['debit']\n",
        "    credit_col = COL_NAMES['Master']['credit']\n",
        "\n",
        "    refined_src_df = src_df[[id_col, debit_col, credit_col]]\n",
        "\n",
        "    differing_transactions_df = pd.DataFrame()\n",
        "\n",
        "    for dst_sheet in dst_sheet_list:\n",
        "        dst_df = get_cols_by_name_from_sheet(\n",
        "            dst_sheet,\n",
        "            [id_col, debit_col, credit_col],\n",
        "            header_row=1\n",
        "        )\n",
        "        dst_df[id_col] = dst_df[id_col].astype(str)\n",
        "        dst_df[debit_col] = dst_df[debit_col].astype(float)\n",
        "        dst_df[credit_col] = dst_df[credit_col].astype(float)\n",
        "\n",
        "        # Merge source and destination DataFrames on the unique ID column\n",
        "        merged_df = pd.merge(refined_src_df, dst_df, on=id_col, suffixes=('_src', '_dst'))\n",
        "\n",
        "        # Identify rows where debit or credit amounts differ\n",
        "        diff_debit = merged_df[merged_df[f'{debit_col}_src'] != merged_df[f'{debit_col}_dst']]\n",
        "        diff_credit = merged_df[merged_df[f'{credit_col}_src'] != merged_df[f'{credit_col}_dst']]\n",
        "\n",
        "        # Combine the differing rows\n",
        "        diff_rows = pd.concat([diff_debit, diff_credit]).drop_duplicates()\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        diff_transactions_df = pd.concat([differing_transactions_df, diff_rows])\n",
        "\n",
        "    return diff_transactions_df\n",
        "\n",
        "\n",
        "@retry_on_quota_exceeded()\n",
        "def problem_with_master_cols(billable_dst_sheet, non_billable_dst_sheet):\n",
        "    \"\"\"\n",
        "    Checks if the columns of the billable and non-billable master sheets match\n",
        "    and if they match the columns in the code dictionary.\n",
        "    Unlike previous error-checking functions, this function prints its own error messages.\n",
        "\n",
        "    Args:\n",
        "        billable_dst_sheet (gspread.models.Worksheet): The billable destination worksheet.\n",
        "        non_billable_dst_sheet (gspread.models.Worksheet): The non-billable destination worksheet.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if there is a problem with the columns, False otherwise.\n",
        "    \"\"\"\n",
        "    billable_headers = set(billable_dst_sheet.row_values(1))\n",
        "    non_billable_headers = set(non_billable_dst_sheet.row_values(1))\n",
        "\n",
        "    if billable_headers != non_billable_headers:\n",
        "        print(f\"❌ Billable and non-billable master sheet columns don't match. Terminating push.\")\n",
        "        print(f\"Billable headers:     {sorted(billable_headers)}\")\n",
        "        print(f\"Non-billable headers: {sorted(non_billable_headers)}\")\n",
        "        return True\n",
        "\n",
        "    code_master_cols = set(COL_NAMES['Master'].values())\n",
        "    actual_master_cols = billable_headers\n",
        "\n",
        "    if code_master_cols != actual_master_cols:\n",
        "        print(f\"❌ Master sheet columns don't match those in code dictionary. Terminating push.\")\n",
        "        print(f\"Code master columns:   {sorted(code_master_cols)}\")\n",
        "        print(f\"Actual master columns: {sorted(actual_master_cols)}\")\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Get Corporate <> Property CoA Mapping sheet\n",
        "    coa_map_df = df_from_spread_url(\n",
        "        CORPORATE_PROPERTY_COA_MAPPING_SHEET['url'],\n",
        "        sheet_title=CORPORATE_PROPERTY_COA_MAPPING_SHEET['sheet title'])\n",
        "\n",
        "    # Create mapping from Property GL codes to corporate accounts\n",
        "    property_code_to_corporate_acc = pd.Series(\n",
        "        coa_map_df['Corporate Description / Chart of Accounts on QBO'].values,\n",
        "        index=coa_map_df['Property GL Code / SKU on QBO']\n",
        "        ).to_dict()\n",
        "    property_code_to_corporate_acc['N/A'] = 'N/A'\n",
        "\n",
        "    # Gets urls and names of all spreadsheets in \"Ready to Upload to Master Sheet\"\n",
        "    src_files_info = get_files_info_from_folder(READY_TO_UPLOAD_TO_MASTER_SHEET_FOLDER_URL)\n",
        "\n",
        "    pushed_sources = []\n",
        "    for src_file_info in src_files_info:\n",
        "        src_filename = src_file_info['name']\n",
        "        src_filetype = src_file_info['type']\n",
        "        src_url = src_file_info['url']\n",
        "        source = src_filename.split('_')[0]\n",
        "\n",
        "        # Ignore Google Colab file\n",
        "        if src_filetype == 'application/vnd.google.colaboratory':\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n⏳Processing {src_filename}...\")\n",
        "\n",
        "        df = df_from_file_url(src_url, src_filetype)\n",
        "\n",
        "        # Get live sheets into a more readable and standard format\n",
        "        if source in LIVE_SHEETS.keys():\n",
        "            # We need to create a new df here bc of the melting and grouping\n",
        "            df = format_live_sheet(df, source)\n",
        "\n",
        "        # Check for missing columns in the raw spreadsheet/csv\n",
        "        missing_col_names = get_missing_raw_cols(df, source)\n",
        "        if len(missing_col_names) > 0:\n",
        "            print(f\"❌ Could not push {src_filename}: missing {source} column(s).\")\n",
        "            print(f\"Missing columns: {[col_name for col_name in missing_col_names]}\")\n",
        "            continue\n",
        "\n",
        "        # Refine the dataframe to just the relevant columns and relable them w/ master sheet column\n",
        "        # names. Add default columns like \"Uploaded to SaasAnt as Bill\" = False or the Source column\n",
        "        # (this is a mutator function)\n",
        "        general_formatting(df, source)\n",
        "\n",
        "        # Apply transformations to finalize the upload-ready df.\n",
        "        # Returns a dict w/ two dfs: billable and non-billable\n",
        "        source_to_master(df, source)\n",
        "\n",
        "        # Open the destination sheets\n",
        "        billable_dst_sheet = open_sheet_from_url(\n",
        "            MASTER_SHEET['url'],\n",
        "            sheet_title=MASTER_SHEET['sheet title']\n",
        "        )\n",
        "        non_billable_dst_sheet = open_sheet_from_url(\n",
        "            NON_BILLABLE_SHEET['url'],\n",
        "            sheet_title=NON_BILLABLE_SHEET['sheet title']\n",
        "        )\n",
        "\n",
        "        # Check that the column names match between billable and non-bllable master sheets and are\n",
        "        # the same as the global COL_NAMES dictionary\n",
        "        if problem_with_master_cols(billable_dst_sheet, non_billable_dst_sheet):\n",
        "            # The error messages are printed inside the function\n",
        "            break\n",
        "\n",
        "        # Check for missing mandatory SaasAnt data\n",
        "        missing_saasant_data = get_missing_saasant_data(df, source)\n",
        "        if len(missing_saasant_data.keys()) > 0:\n",
        "            print(f\"❌ Could not push {src_filename}: missing mandatory SaasAnt data.\")\n",
        "            print(f\"Missing data: {[item for item in missing_saasant_data.items()]}\")\n",
        "            continue\n",
        "\n",
        "        # Check for duplicates\n",
        "        duplicates = get_duplicates(df, [billable_dst_sheet, non_billable_dst_sheet])\n",
        "\n",
        "        if source in LIVE_SHEETS.keys():\n",
        "            # Check that previously pushed transactions still match the live sheet\n",
        "            differing_transactions = get_differing_transactions(\n",
        "                df,\n",
        "                [billable_dst_sheet, non_billable_dst_sheet]\n",
        "            )\n",
        "            if not differing_transactions.empty:\n",
        "                print(f\"🛑 WARNING - Could not push {src_filename}: found previously pushed \"\n",
        "                      \"transactions in the master sheet that no longer match their source sheet.\")\n",
        "                print(f\"Differing transactions:\\n{differing_transactions}\")\n",
        "                continue\n",
        "            # Only upload new transactions from Marketing Billbacks (remove rows with duplicate IDs)\n",
        "            rows_to_drop = df.index[df[COL_NAMES['Master']['unique id']].isin(duplicates)].tolist()\n",
        "            df.drop(rows_to_drop, inplace=True)\n",
        "            print(f\"{src_filename} has {df.shape[0]} new transactions that will be pushed\")\n",
        "        elif duplicates:\n",
        "            print(f\"❌ Could not push {src_filename}: duplicates found.\")\n",
        "            print(f\"Duplicate IDs: {duplicates}\")\n",
        "            continue\n",
        "\n",
        "        # Return billback and non-billback dfs\n",
        "        billable_filter = (df[COL_NAMES['Master']['property name']].isin(PROPERTY_NAMES))\n",
        "        master_billbacks_df = df[billable_filter]\n",
        "        master_non_billbacks_df = df[~billable_filter]\n",
        "\n",
        "        # Push to master sheets\n",
        "        append_df_to_sheet(master_billbacks_df, billable_dst_sheet)\n",
        "        append_df_to_sheet(master_non_billbacks_df, non_billable_dst_sheet)\n",
        "        print(f\"✅ Pushed {src_filename}.\")\n",
        "        pushed_sources.append(source)\n",
        "\n",
        "        if RELOCATE_SPREAD_MODE:\n",
        "            move_file(src_url, ARCHIVED_CODED_FILES_FOLDER_URL)\n",
        "\n",
        "    pushed_sources.sort()\n",
        "    total_sources = sorted(source for source in COL_NAMES.keys())\n",
        "    total_sources.remove('Master')\n",
        "    print(\"\\n\")\n",
        "    print(f\"Sources pushed: \"\n",
        "          f\"{', '.join(src if src in pushed_sources else '_'*len(src) for src in total_sources)}\")\n",
        "    print(f\"Total sources:  {', '.join(total_sources)}\")\n"
      ],
      "metadata": {
        "id": "L-ADV39M-J1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "d9e19647-46da-4c1b-e88e-f48412a72991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-65e46c43edc5>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m from pandas.io.api import (\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mExcelFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mto_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0;31m from pandas.io.pytables import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mHDFStore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mread_hdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m from pandas.core.computation.pytables import (\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mPyTablesExpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mmaybe_expression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_get_module_lock\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNh-nXsFmdyO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}